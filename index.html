<!doctype html>
<meta charset="utf-8">
<head>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js"></script>
</head>
<script src="https://distill.pub/template.v1.js"></script>
<!-- load the d3.js library -->    
<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min.js"></script>
<!-- <link rel="stylesheet" type="text/css" href="styles.css"> -->

  <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderEquations();
    });

    function renderEquations() {
      var contentElement = document.getElementById("content");
      var equations = contentElement.innerHTML.match(/\$\((.*?)\)\$/g); // Find equations in the content

      equations.forEach(function(equation) {
        var equationText = equation.substring(2, equation.length - 2); // Remove the '$(' and ')$' wrappers
        var renderedEquation = katex.renderToString(equationText); // Render the equation using KaTeX

        contentElement.innerHTML = contentElement.innerHTML.replace(equation, renderedEquation); // Replace the equation with the rendered version
      });
    }
  </script>


<style>
path { 
    stroke: steelblue;
    stroke-width: 2;
    fill: none;
}
.axis path,
.axis line {
    fill: none;
    stroke: grey;
    stroke-width: 1;
    shape-rendering: crispEdges;
}
.math-details {
    padding-left: 10em;
    padding-right: 10em;
    padding-bottom: 0em;
    padding-top: 1em;
    margin-bottom: 1em;
}
.shaded-figure {
    background-color: hsl(0, 0%, 97%);
    border-top: 1px solid hsla(0, 0%, 0%, 0.1);
    border-bottom: 1px solid hsla(0, 0%, 0%, 0.1);
    padding: 20px 20px;
    width: 800px;
}
.caption {
  font-size: 14px;
  text-align: center;
}
</style>



<script type="text/front-matter">
  title: "Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task"
  description: ""
  authors:
  - Maya Okawa: https://github.com/mayaokawa 
  affiliations:
  - NTT Research: https://ntt-research.com/ 
</script>


<dt-article>
  <h1>Exploring Diffusion Models on a Synthetic Task</h1>
  <h2>Compositional Abilities Emerge Multiplicatively</h2>
  <dt-byline></dt-byline>
  <p> In this blog post, we will introduce our recent research in which we evaluated the capabilities of diffusion models<dt-cite key="gregor2015draw"></dt-cite>. These models have already demonstrated their potential to generate realistic images. Our investigation zeroes in on text-to-image generation, aiming to assess the model's compositional capabilities.
To facilitate this evaluation, we first developed a theoretical framework that we've named 'concept graph'. This framework provided a clear definition of what we define 'compositional ability'. Subsequently, we devised a synthetic task in alignment with our theoretical framework. We then empirically examined the performance of the diffusion model on this task, providing us with a comprehensive understanding of its compositional ability. </p>

  <h2>Compositional Ability of the Diffusion Model</h2>
  <p> The diffusion models have demonstrated impressive capabilities in generating realistic images. However, their ability to integrate learned concepts and generate entirely new images remains a topic of ongoing discussion. The following images serve as examples of those generated by a diffusion model. In these cases, we provided color and object descriptors via text prompts and tasked the model with generating images of specific animals or fishes.
  <img src="images/lfp_temp.png" alt="(Lack of) Compositionality in text-conditioned diffusion models. " style="width:800px" hspace="30" vspace="30"/>
The model showed its power in generating accurate images for lizards and goldfish, accurately adhering to the text prompts. However, its performance was less successful when tasked with generating images of a panda. Here, the model disregarded the color information specified in the text prompt, resulting in an incorrect output. These observations lead us to an intriguing question: When does the diffusion model succeed in compositing multiple concepts (in this case, color and object), and when does it fail? </p>

  <h2>Synthetic Task</h2>
  <p> To delve into this question empirically, we designed a synthetic task. More specifically, we constructed a synthetic dataset designed to test the model's compositional abilities. The nature of this task resembles an IQ test. Let's look at the simplest form of this task.
We assembled a set of image and text prompt pairs. The model is trained on a training set (as shown on the left), following which it generates a new image based on the provided text prompts.  </p>
  <figure>
   <figcaption>
   Here's an interactive challenge: Can you guess which object/color should be in the blank space <img src="images/concept_graph_test_icon.png" alt="Test" style="width:20px"/> ? You can see the answer by hovering the mouse over the image.
   </figcaption>
  </figure>
  <p style="text-align:center;"><img src="images/graph_3d_test.png" onmouseover="this.src='images/graph_3d_answer.png';" title="The correct answer is a 'blue circle'." alt="Test" style="width:400px"/></p>

  <p> When trying to solve this test, we make associations between each character in the text prompt and specific attributes in the image. We guess that numbers correlate to the shape of the object (i.e., triangle or circle), while alphabetical characters are tied to its color (i.e., blue or red). </p>
  <p style="text-align:center;"><img src="images/graph_3d_inference.png" alt="Inference" style="width:420px"/></p>
  <p> Following this, we imagine an unseen image by combining these learned concepts - in this case, shape and color. <br><br> </p>

  <h2>Concept Graph: A Minimalistic Framework for Compositionality </h2>

  <div class="container" style="width: 1200px;">
  <div class="row">
  <div class="column">
  <p> To structure this test, we devised a theoretical framework. Within this framework, we represented the compositional ability using a graph. For example, let's consider adding an additional concept, 'size' (either small or big), to the above example. This results in a 3D cube graph, visually embodying these three distinct concepts. </p>
  <figure>
   <figcaption style="grid-column: kicker">
    The edges represent the concepts (shape, color, size), while the nodes denote their respective attributes (triangle/circle, red/blue, small/big). The proposed concept graph shares a similar structure with the Hamming Graph and is also inspired by psychological studies. 
   </figcaption>
  </figure>
  </div>
  <div class="column">
  <img src="images/concept_graph_3d.png" alt="3d concept graph" style="width:220px"/>
  </div>
  </div>
  </div>

  <div class="math-details shaded-figure">
  <b> Details for Interested Rearders</b>
  <div id="content">

  <i> Definition 1 (Concept Variables.) </i>
  <div class="container" style="display: flex; align-items: center;">
  <p><img src="images/concept_graph_definition1.png" alt="" class="left" style="width:150px; margin-right: 20px;"/> </p> 
  <p> Let $(V = \{v_1, v_2, ..., v_n\})$ be a set of $(n)$ concept variables, where each $(v_i)$ represents a specific property of an object. </p>
  </div>

  <i> Definition 2 (Concept Values.) </i>
  <div class="container" style="display: flex;">
  <p><img src="images/concept_graph_definition2.png" alt="" class="left" style="width:150px; margin-right: 20px;"/> </p> 
  <p> For each concept variable $(v_i \in V)$, let $(C_i = \{c_{i1}, c_{i2}, ..., c_{ik_i}\})$ be the set of $(k_i)$ possible values that $(v_i)$ can take. Each element of the set $(C_{i})$ is called a concept value. </p>
  </div>

  <i> Definition (Concept Distance.) </i>
  <p> Given two concept classes $(C^{(1)} = (c^{(1)}_{1}, c^{(1)}_{2}, ..., c^{(1)}_{n}))$ and $(C^{(2)} = (c^{(2)}_{1}, c^{(2)}_{2}, ..., c^{(2)}_{n}))$, the concept distance $(d(C^{(1)}, C^{(2)}))$ is defined as the number of elements that differ between the two concept classes: <p> 
  <p style="text-align: center;"> $( d(C^{(1)}, C^{(2)}) = \sum_{i=1}^{n} I(c^{(1)}_{i}, c^{(2)}_{i}) )$ </p>

  </div>
  </div>

    


  <h2>Multiplicative Emergence of Compositional Abilities </h2>
  <div class="container" style="width: 1200px;">
  <div class="row">
  <div class="column">
  <p> Based on this theoretical model, we can formulate a hypothesis: In the learning process, the distance from the training set on the graph dictates when the capability emerges. Let's consider using the blue nodes for training and the pink nodes for testing. </p>
  </div>
  <div class="column">
  <p> <img src="images/concept_graph_training.png" alt="Concept distance" style="width:220px"/></p>
  </div>
  </div>
  </div>

  <p>Intuitively, the compositional ability emerges in sequence: The model first generates the correct images for the light pink nodes; subsequently, it generates accurate images for the dark pink nodes. Is this hypothesis correct? We'll verify it through an experiment. </p>
  <p> Below we visualize the progression of the accuracy for the generated images throughout the training period.  </p>
<figure>

  <figure class="subgrid">
   <figcaption style="grid-column: kicker"> In this experiment, we trained linear classifiers for each concept - shape, color, size - using pairs of text and images from both the training and test datasets. Subsequently, we evaluated their classification accuracy for the generated images. </figcaption>
  </figure>
  <!-- <script src="js/animated_v3.js"></script> -->
  <div id="chartContainer1" style="height: 300px; width: 500px; margin: 0px auto;"></div>
  <script src="js/figure_1.js"></script>

  <p>In this scenario, the diffusion model demonstrates compositional capability, achieving 100% accuracy for both the 1-hop (light pink) and the 2-hop (dark pink) nodes. This compositional ability unfolds sequentially: first, the 1-hop node reaches 100% accuracy, followed by the 2-hop node achieving full accuracy after a certain period. This experimental result seems intuitive, as the model can synthesize two concepts once each individual concept has been learned. Interestingly, there's the emergence of accuracy for the 2-hop node. </p>


  <figure>
   <figcaption>
   What is the mechanism behind emergent capability? To investigate this, we have plotted the accuracy for each concept. 
   </figcaption>
  </figure>


  <div id="chartContainer2" style="height: 300px; width: 500px; margin: 0px auto;"></div>
  <script src="js/figure_2.js"></script>
  <!-- <p style="text-align:center;"><img src="Multiplicative_Emergence_Data.png" alt="Multiplicity underlies the sudden emergence of compositional capabilities." style="width:500px"/></p> -->
  <p>Our task is multiplicative in nature, meaning that the accuracy reaches 100% only when all concepts are accurate. Therefore, the increase in accuracy commences after learning the most challenging concept, which in this case, is color. </p>

  <p>The aforementioned results can be viewed from the perspective of fairness. We noticed a bias towards red, the majority color in the training set. The minority color, which is blue, was learned only after a certain period following the learning of the majority color, red. </p>
  <!-- <p style="text-align:center;"><img src="Color_learning_dynamics.png" alt="Delayed emergence of abilities to generate minority colors for distant classes. " style="width:480px"/></p> -->
  <div id="chartContainer3" style="height: 300px; width: 500px; margin: 0px auto;"></div>
  <script src="js/figure_3.js"></script>
  <p>This result suggests that we should not halt training once the loss function converges for the training datasets. Continuing beyond this point can potentially yield fairer results for the minority groups. </p>


  <h2>Challenges for Compositional Generalization </h2>
  <p> Next, we will investigate examples where the diffusion model struggles to composite multiple concepts. We decreased the number of samples for specific nodes and monitored the accuracy of the images generated for varying sample sizes.  </p>

  <div class="container" style="display: flex; align-items: center;">
  <p><img src="images/frequency_data_0.png" style="width:100px" /></p>
  <p><img src="images/frequency_data_1.png" style="width:100px" /></p>
  <div id="mini_chartContainer4" style="height: 250px; width: 350px; font-size: 17px; margin: 0px auto;"></div>
  <script src="js/figure_4.js"></script>
  <!-- <p><img src="frequency_data_2.png" style="width:300px" /></p> -->
  <p><img src="images/frequency_data_3.png" style="width:100px" /></p>
  <!-- <p><img src="frequency_data_4.png" style="width:300px" /></p> -->
  <div id="mini_chartContainer5" style="height: 250px; width: 350px; font-size: 17px; margin: 0px auto;"></div>
  <script src="js/figure_5.js"></script>
  </div>

  <p>We see that different concepts require varying amounts of samples: the 'color' concept necessitates fewer samples, while the 'size' concept requires a larger quantity. </p>

  <h2>Avenues for Extensions </h2>
  <ul>
  <li> We will extend the theoretical foundation of 'concept graphs' framework. </li>
  <li> We will conduct experiments using real-world data. </li> 
  </ul>

  <h2>Wrapping Up </h2>
  <p>In this blog post, we introduce our theoretical framework, termed 'concept graph', which is designed to assess the compositional abilities of text-to-image generation models. Based on this theoretical framework, we created synthetic datasets and conducted a series of experiments.

In this post, we highlight some of our key experimental results. Our findings are as follows:
(i) The compositional structure of the data-generating process determines the order in which the capabilities emerge, as well as the ability to combine them;
(ii) The learning of individual concepts influences performance on compositional tasks. This has a multiplicative effect, explaining the sudden emergence of capability; and (iii) When the training data for specific concepts is insufficient, learning and composing capabilities become more challenging. </p>

</dt-article>

<dt-appendix>
</dt-appendix>


<script type="text/bibliography">
  @article{maya2023concept,
    title={Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task},
    author={Maya Okawa, Ekdeep Singh Lubana, Hidenori Tanaka},
    journal={arXivreprint arXiv:},
    year={2023},
    url={https://arxiv.org/pdf/.pdf}
  }
</script>
