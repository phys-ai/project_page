I will talk about our current research on analyzing the generalization ability of the diffusion models. 

As you all know diffusion model shows its power to generate realistic images. When we put text as prompt, the model output the images that correspond to these texts. Here, we provided color and object descriptors via text prompts and tasked the model with generating images of specific objects like white-colored lizards, green-colored fish. The model generates accurate images for lizards and goldfish; it adhers to the text prompts. We can think that the diffusion model learned concepts, here color and object, from training data, and make new images by combining these concepts. 

However, it's not for panda. Here, the model disregards the color information specified in the text prompt and gives incorrect outputs.
These observations lead us to a question: When does the diffusion model succeed in compositing multiple concepts like color and object, and when does it fail?

Our resarch answer this question empirically. To do so, we designed a synthetic task. We constructed a synthetic dataset designed to test the model's compositional abilities. The nature of this task resembles an IQ test. Here's the simplest form of this task.
We have a set of image and text prompt pairs. The model is trained on the left training set, and then it generates a new image based on the provided text prompts.Can you guess which object/color should be in the blank space? 
 

